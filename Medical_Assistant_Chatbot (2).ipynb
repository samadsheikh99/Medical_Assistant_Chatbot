{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ],
      "metadata": {
        "id": "GEcBBb-rHo7r"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWVCxEWX6rEx",
        "outputId": "5c001d77-fdc3-434b-f552-1d31362e45db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: pydantic 2.11.7 does not provide the extra 'dotenv'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m152.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m166.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [streamlit]\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9/9\u001b[0m [langchain-community]\n",
            "\u001b[1A\u001b[2K"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade pip\n",
        "!pip install -q fastapi uvicorn nest-asyncio pydantic[dotenv]\n",
        "!pip install -q streamlit requests httpx python-multipart\n",
        "!pip install -q langchain langchain-community langchain-huggingface\n",
        "!pip install -q pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoTokenizer\n",
        "from unsloth import FastLanguageModel\n",
        "import uvicorn, nest_asyncio, threading\n",
        "from pyngrok import ngrok\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain_core.runnables import RunnableSequence, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOZluFiZ2DYj",
        "outputId": "b04014f0-60da-4652-d53d-3ac58eed5670"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1-310415437.py:6: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastLanguageModel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN = \"YOUR HUGGINGFACE TOKEN\"  # <-- REPLACE THIS\n",
        "login(token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "ygNCxuAo2OdU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Model configuration\n",
        "MODEL_ID = \"abdulsamad99/medical-fine-tuning\"\n",
        "MAX_SEQ_LENGTH = 512\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model, _ = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_ID,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True\n",
        ")\n",
        "model = FastLanguageModel.for_inference(model).to(\"cuda\")\n",
        "\n",
        "print(\"âœ… Model loaded successfully on GPU:\", torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUY0Japn2fi0",
        "outputId": "627832e6-11bc-4f99-a0e7-7b2c384b3b77"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.7.5: Fast Llama patching. Transformers: 4.53.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.7.5 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model loaded successfully on GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK_TOKEN = \"YOUR NGROK TOKEN\"  # <-- your token\n",
        "ngrok.set_auth_token(NGROK_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STUxrFW525wv",
        "outputId": "02d2fe60-e8fe-4535-e941-f5eae5d5b3fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Per-session version (REPLACES global chat_history & chain) ======\n",
        "import re, torch\n",
        "from collections import defaultdict\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain_core.runnables import RunnableSequence, RunnableLambda\n",
        "\n",
        "SESSION_HISTORIES = defaultdict(list)  # session_id -> list[Messages]\n",
        "\n",
        "RE_THINK = re.compile(r\"<think>(.*?)</think>\", re.DOTALL | re.IGNORECASE)\n",
        "RE_RESPONSE = re.compile(r\"<response>(.*?)</response>\", re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "def format_history(session_id: str, max_turns: int = 5) -> str:\n",
        "    msgs = SESSION_HISTORIES[session_id]\n",
        "    if max_turns:\n",
        "        msgs = msgs[-(max_turns*2):]\n",
        "    lines = []\n",
        "    for m in msgs:\n",
        "        if isinstance(m, HumanMessage):\n",
        "            lines.append(f\"Doctor: {m.content.strip()}\")\n",
        "        else:\n",
        "            lines.append(f\"Assistant: {m.content.strip()}\")\n",
        "    return \" | \".join(lines)\n",
        "\n",
        "def build_prompt_from_inputs(inputs: dict) -> str:\n",
        "    session_id = inputs[\"session_id\"]\n",
        "    symptom = inputs[\"symptom\"]\n",
        "    ctx = format_history(session_id, max_turns=3)\n",
        "    ctx_line = f\"Context: {ctx}\\n\" if ctx else \"\"\n",
        "    return f\"{ctx_line}Question: {symptom}\\n<think>\"\n",
        "\n",
        "def llama_generate(prompt: str) -> str:\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH).to(\"cuda\")\n",
        "    input_len = enc[\"input_ids\"].shape[1]\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=450,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    gen_tokens = out[0, input_len:]\n",
        "    return tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "def run_model_and_merge(prompt: str) -> str:\n",
        "    gen = llama_generate(prompt)\n",
        "    return prompt + gen\n",
        "\n",
        "def parse_full_text(full_text: str) -> dict:\n",
        "    think_m = RE_THINK.search(full_text)\n",
        "    resp_m = RE_RESPONSE.search(full_text)\n",
        "    think = think_m.group(1).strip() if think_m else \"\"\n",
        "    diagnosis = resp_m.group(1).strip() if resp_m else full_text.strip()\n",
        "    return {\"think\": think, \"diagnosis\": diagnosis}\n",
        "\n",
        "llm_chain = RunnableSequence(\n",
        "    RunnableLambda(build_prompt_from_inputs)\n",
        "    | RunnableLambda(run_model_and_merge)\n",
        "    | RunnableLambda(parse_full_text)\n",
        ")\n",
        "print(\"âœ… Per-session chain ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpMiFPwexk4g",
        "outputId": "f5439b07-47a6-4fb1-c062-f112772fa0e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Per-session chain ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= FastAPI (Option B: per-session) =================\n",
        "import nest_asyncio, threading, uvicorn, time\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = FastAPI(title=\"Medical Diagnosis API (Per Session)\")\n",
        "\n",
        "class SymptomRequest(BaseModel):\n",
        "    symptom: str\n",
        "    session_id: str\n",
        "\n",
        "@app.post(\"/diagnose\")\n",
        "async def diagnose(req: SymptomRequest):\n",
        "    try:\n",
        "        SESSION_HISTORIES[req.session_id].append(HumanMessage(content=req.symptom))\n",
        "        result = llm_chain.invoke({\"session_id\": req.session_id, \"symptom\": req.symptom})\n",
        "        SESSION_HISTORIES[req.session_id].append(AIMessage(content=result[\"diagnosis\"]))\n",
        "        return {\"diagnosis\": result[\"diagnosis\"], \"think\": result[\"think\"]}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "def run_api():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "threading.Thread(target=run_api, daemon=True).start()\n",
        "time.sleep(2)\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(\"FastAPI running:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_ZPZA2kxv5r",
        "outputId": "f1d86e59-6d6e-4504-a8b6-6f026e54b44b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [7084]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI running: https://0d6b4b562202.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= Streamlit (Option B) =================\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(f\"\"\"\n",
        "import streamlit as st\n",
        "import requests, os\n",
        "\n",
        "API_URL = \"{public_url}/diagnose\"\n",
        "\n",
        "st.set_page_config(page_title=\"Medical Diagnosis Assistant\", page_icon=\"ğŸ©º\")\n",
        "st.title(\"ğŸ©º Medical Diagnosis Assistant (Per Session)\")\n",
        "\n",
        "if \"session_id\" not in st.session_state:\n",
        "    st.session_state.session_id = os.urandom(8).hex()\n",
        "if \"chat\" not in st.session_state:\n",
        "    st.session_state.chat = []\n",
        "\n",
        "symptoms = st.text_area(\"Enter symptoms / question:\", height=120)\n",
        "show_think = st.checkbox(\"Show reasoning (<think>)\", value=False)\n",
        "\n",
        "if st.button(\"Get Diagnosis\"):\n",
        "    if symptoms.strip():\n",
        "        resp = requests.post(API_URL, json={{\"symptom\": symptoms, \"session_id\": st.session_state.session_id}})\n",
        "        if resp.status_code == 200:\n",
        "            data = resp.json()\n",
        "            st.session_state.chat.append((\"Doctor\", symptoms))\n",
        "            st.session_state.chat.append((\"Assistant\", data.get(\"diagnosis\",\"[No diagnosis]\"), data.get(\"think\",\"\")))\n",
        "        else:\n",
        "            st.error(f\"Error {{resp.status_code}}: {{resp.text}}\")\n",
        "    else:\n",
        "        st.warning(\"Please enter something first.\")\n",
        "\n",
        "st.subheader(\"Conversation\")\n",
        "for msg in st.session_state.chat:\n",
        "    if msg[0] == \"Doctor\":\n",
        "        st.markdown(f\"**Doctor:** {{msg[1]}}\")\n",
        "    else:\n",
        "        st.markdown(f\"**Assistant:** {{msg[1]}}\")\n",
        "        if show_think and len(msg) > 2 and msg[2]:\n",
        "            with st.expander(\"Reasoning\"):\n",
        "                st.write(msg[2])\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    if st.button(\"New Session\"):\n",
        "        st.session_state.session_id = os.urandom(8).hex()\n",
        "        st.session_state.chat = []\n",
        "        st.success(\"Started a new session.\")\n",
        "with col2:\n",
        "    if st.button(\"Clear Chat\"):\n",
        "        st.session_state.chat = []\n",
        "\"\"\")\n",
        "\n",
        "import subprocess, time\n",
        "proc = subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"])\n",
        "time.sleep(5)\n",
        "streamlit_url = ngrok.connect(8501).public_url\n",
        "print(\"Streamlit running:\", streamlit_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltidtOISx09m",
        "outputId": "f3ed016f-3b1c-4acf-c355-b16f1553a178"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit running: https://e53d6b620d15.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}